{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOysXWvIbYJxOISfxUEh4el",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ltejadavic/EdgarDB/blob/main/EdgarDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNRHkHjc2UKg",
        "outputId": "40c34f91-f6a9-42e2-9c51-9483b928e57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched data from https://data.sec.gov/submissions/CIK0000789019.json\n",
            "ITEM 1A not found.\n",
            "ITEM 1A not found.\n",
            "Fetched data from https://data.sec.gov/submissions/CIK0000910521.json\n",
            "Fetched data from https://data.sec.gov/submissions/CIK0000772406.json\n",
            "Fetched data from https://data.sec.gov/submissions/CIK0001443646.json\n",
            "Results saved to covid_mentions_item1a.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import csv\n",
        "\n",
        "\n",
        "def fetch_metadata(cik):\n",
        "    \"\"\"\n",
        "    Fetch metadata for the given CIK from the SEC submissions API.\n",
        "    Args:\n",
        "        cik (str): Central Index Key (CIK) of the entity.\n",
        "    Returns:\n",
        "        dict: JSON metadata of the filings for the entity.\n",
        "    \"\"\"\n",
        "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "    headers = {'User-Agent': \"getbagsfinance@gmail.com\"}  # Required user-agent header for SEC API\n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(f\"Fetched data from {url}\")\n",
        "    return response.json()  # Return the JSON metadata\n",
        "\n",
        "\n",
        "def get_10k_urls(metadata):\n",
        "    \"\"\"\n",
        "    Extract 10-K filing URLs from the metadata JSON for years 2020-2024.\n",
        "    Args:\n",
        "        metadata (dict): JSON metadata of the filings for the entity.\n",
        "    Returns:\n",
        "        list: List of tuples containing filing URLs and their corresponding years.\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    recent_filings = metadata.get('filings', {}).get('recent', {})\n",
        "    for i, form_type in enumerate(recent_filings.get('form', [])):\n",
        "        if form_type == '10-K':  # Only consider 10-K filings\n",
        "            accession_number = recent_filings['accessionNumber'][i].replace('-', '')  # Clean accession number\n",
        "            primary_document = recent_filings['primaryDocument'][i]  # Primary document file name\n",
        "            cik = metadata['cik']\n",
        "            year = int(recent_filings['filingDate'][i][:4])  # Extract the year from the filing date\n",
        "            if 2020 <= year <= 2024:  # Filter filings between 2020 and 2024\n",
        "                filing_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}/{primary_document}\"\n",
        "                urls.append((filing_url, year))  # Append the URL and year\n",
        "    return urls\n",
        "\n",
        "\n",
        "def fetch_filing_html(url):\n",
        "    \"\"\"\n",
        "    Fetch the HTML content of a filing from the provided URL.\n",
        "    Args:\n",
        "        url (str): URL of the filing.\n",
        "    Returns:\n",
        "        bytes: HTML content of the filing.\n",
        "    \"\"\"\n",
        "    headers = {'User-Agent': \"getbagsfinance@gmail.com\"}  # Required user-agent header for SEC API\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.content  # Return the HTML content\n",
        "\n",
        "\n",
        "def extract_item_1a_section(html_content):\n",
        "    \"\"\"\n",
        "    Extract the ITEM 1A section from the HTML content using BeautifulSoup.\n",
        "    Args:\n",
        "        html_content (bytes): HTML content of the filing.\n",
        "    Returns:\n",
        "        str: Extracted text of the ITEM 1A section, or None if not found.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    item_1a = soup.find(string=re.compile(r'ITEM\\s*1A[^A-Za-z]*Risk Factors', re.IGNORECASE))  # Locate the \"ITEM 1A\" header\n",
        "\n",
        "    if not item_1a:\n",
        "        print(\"ITEM 1A not found.\")\n",
        "        return None\n",
        "\n",
        "    # Extract text from ITEM 1A to the next section header\n",
        "    section_text = []\n",
        "    for sibling in item_1a.find_all_next(string=True):\n",
        "        if re.search(r'(ITEM\\s*\\d+\\s|PART\\s*II)', sibling, re.IGNORECASE):  # Stop at the next section or part\n",
        "            break\n",
        "        section_text.append(sibling)\n",
        "\n",
        "    return ' '.join(section_text).strip()  # Join all extracted lines into a single string\n",
        "\n",
        "\n",
        "def search_keywords(section_text, keywords):\n",
        "    \"\"\"\n",
        "    Search for keywords in the extracted section and return sentences containing them.\n",
        "    Args:\n",
        "        section_text (str): Text content of the ITEM 1A section.\n",
        "        keywords (list): List of keywords to search for.\n",
        "    Returns:\n",
        "        list: List of sentences containing the keywords.\n",
        "    \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?]) +', section_text)  # Split text into sentences\n",
        "    related_sentences = [s for s in sentences if any(keyword.lower() in s.lower() for keyword in keywords)]  # Filter sentences containing keywords\n",
        "    return related_sentences\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # List of CIKs to process\n",
        "    ciks = ['0000789019', '0000910521', '0000772406', '0001443646']\n",
        "\n",
        "    # Keywords to search for in the ITEM 1A section\n",
        "    covid_keywords = [\"COVID-19\", \"pandemic\", \"Corona\"]\n",
        "\n",
        "    # List to store results\n",
        "    results = []\n",
        "\n",
        "    # Loop through each CIK\n",
        "    for cik in ciks:\n",
        "        metadata = fetch_metadata(cik)  # Fetch metadata for the CIK\n",
        "        filing_urls = get_10k_urls(metadata)  # Get 10-K URLs for the entity\n",
        "\n",
        "        # Process each filing URL\n",
        "        for url, year in filing_urls:\n",
        "            html_content = fetch_filing_html(url)  # Fetch the HTML content of the filing\n",
        "            item_1a_text = extract_item_1a_section(html_content)  # Extract the ITEM 1A section\n",
        "\n",
        "            if item_1a_text:  # If the section is found\n",
        "                total_words = len(item_1a_text.split())  # Count the total words in the section\n",
        "                covid_sentences = search_keywords(item_1a_text, covid_keywords)  # Search for COVID-related sentences\n",
        "\n",
        "                # Append the results\n",
        "                results.append({\n",
        "                    \"CIK\": cik,\n",
        "                    \"Year\": year,\n",
        "                    \"Total Words in ITEM 1A\": total_words,\n",
        "                    \"Corona Related Sentences\": \" | \".join(covid_sentences) if covid_sentences else \"\"\n",
        "                })\n",
        "\n",
        "    # Write the results to a CSV file\n",
        "    csv_file = \"covid_mentions_item1a.csv\"\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"CIK\", \"Year\", \"Total Words in ITEM 1A\", \"Corona Related Sentences\"])\n",
        "        writer.writeheader()  # Write the CSV header\n",
        "        writer.writerows(results)  # Write the data rows\n",
        "\n",
        "    print(f\"Results saved to {csv_file}\")"
      ]
    }
  ]
}